{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classifier.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkj26/textClassifier/blob/master/webScraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Pvj8SvCSzW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "!pip install selenium"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSnux9wWknaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from string import ascii_uppercase\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayeOOWVs-H0N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmpCxn34bZRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ports = []\n",
        "vessels = []\n",
        "companies = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTBJp7u2mVda",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp = 'https://directories.lloydslist.com/port-browse-name/searchid/0/searchchar/'  #This part is link of the search query that is constant so I decided to hardcode it\n",
        "for c in ascii_uppercase:\n",
        "  i = 1\n",
        "  url = temp + c # Go over all the letters in alphabets and store the ports\n",
        "  wd = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "  while(True):              # As each page has different number of results, using try and except to break the loop when there are no results left to store.\n",
        "    try:\n",
        "      if i == 1:\n",
        "        wd.get(url=url)\n",
        "        souplevel = BeautifulSoup(wd.page_source, 'lxml')\n",
        "      else:\n",
        "        button = wd.find_element_by_link_text(str(i))\n",
        "        button.click()\n",
        "        souplevel = BeautifulSoup(wd.page_source, 'lxml')\n",
        "\n",
        "      for child in souplevel.find_all('a', href=re.compile('port-browse-listing/portid')): #All ports are stored using this template therefore used regular expression to find all <a href = ''></a> tags and stored their text. \n",
        "        if child.text != 'Learn More':\n",
        "          ports.append(child.text) \n",
        "    \n",
        "      i+=1\n",
        "    except:\n",
        "      break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnYyarQ8A6rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NO_OF_PAGES=25 #Only 25 pages to view therefore hard coded it.\n",
        "wd = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "url = 'https://www.marinetraffic.com/en/data/?asset_type=vessels&columns=flag,shipname,photo,recognized_next_port,reported_eta,reported_destination,current_port,imo,ship_type,show_on_live_map,time_of_latest_position,lat_of_latest_position,lon_of_latest_position'\n",
        "wd.get(url=url)\n",
        "for i in range(NO_OF_PAGES):\n",
        "  button = WebDriverWait(wd, 20).until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"reporting_ag_grid\"]/div/div[2]/div[3]/div/div/div/div/div[3]/button[2]')))\n",
        "  if i!=0:\n",
        "    button.click()\n",
        "  soup = BeautifulSoup(wd.page_source, 'lxml')\n",
        "  for element in soup.find_all('a', href=re.compile('/en/ais/details/ships/')): #Similar thought as the for port web scrapper.\n",
        "    vessels.append(element.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3djI4lRVOtzY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = 'https://opencorporates.com/registers'\n",
        "wd = webdriver.Chrome('chromedriver',options=chrome_options)\n",
        "wd.get(url)\n",
        "soup = BeautifulSoup(wd.page_source, 'lxml')\n",
        "numberOfRegistries = int(soup.find(id='DataTables_Table_0_info').text.split(' ')[-2]) #Finding out the number of registries\n",
        "for i in range(numberOfRegistries):\n",
        "  xpath = '//*[@id=\"DataTables_Table_0\"]/tbody/tr['+str(i+1)+']/td[3]/a' #Every row of the table and third column which is a clickable link to the companies.\n",
        "  companiesLink = WebDriverWait(wd, 20).until(EC.presence_of_element_located((By.XPATH, xpath)))\n",
        "  companiesLink.click()\n",
        "  lookup = str(wd.current_url)[26:]+'/'  #Lookup for the directory link which is used for every company in the registry\n",
        "\n",
        "  for j in range(3): \n",
        "    if j != 0:\n",
        "      nextPath = '//*[@id=\"results\"]/div/div[1]/ul/li[8]/a'  #XPath for the next button\n",
        "      nextButton = wd.find_element_by_xpath(nextPath)\n",
        "      nextButton.click()\n",
        "        \n",
        "    subSoup = BeautifulSoup(wd.page_source, 'lxml')\n",
        "    for k in subSoup.find_all('a', href=re.compile(lookup)):\n",
        "      companies.append(k.text)\n",
        "  wd.get(url)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUuWGjPVQ8h5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ports = np.array(ports)\n",
        "vessels = np.array(vessels)\n",
        "companies = np.array(companies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRk5ihlCucwo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "330b1662-643a-497d-c4ee-145c3043c80e"
      },
      "source": [
        "print(len(ports))\n",
        "print(len(vessels))\n",
        "print(len(companies))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6657\n",
            "500\n",
            "12512\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}